{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd0a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65",
      "display_name": "Python 3.8.2 64-bit ('HW3': venv)"
    },
    "metadata": {
      "interpreter": {
        "hash": "a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65"
      }
    },
    "colab": {
      "name": "problem_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb3v-7vPca6E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import Module, GRU, Embedding, Linear, Sigmoid, CrossEntropyLoss"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2vJl0i5ca6V"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHjwy_3zcpui",
        "outputId": "9fe5d0e8-e32c-40c0-87fb-fcca9978f63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrqY84fca6V"
      },
      "source": [
        "\"data/sentiment_analysis/train_pos_merged.txt\"\n",
        "# function clearing HTML tags from text\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "# preprocessing\n",
        "def clean_text(path):\n",
        "    reviews = []\n",
        "    all_words = []\n",
        "    with open(path) as pos:\n",
        "        lines = pos.readlines()\n",
        "        for line in lines:\n",
        "            #clear html tags\n",
        "            line = cleanhtml(line)\n",
        "            # lower case and punctuation\n",
        "            line = re.sub(r'[^a-zA-Z]', ' ', line.lower())\n",
        "            # split to list of words\n",
        "            words = line.split()\n",
        "            # add list to reviews\n",
        "            reviews.append(words)\n",
        "            # extend words with new review\n",
        "            all_words.extend(words)\n",
        "\n",
        "    return reviews, all_words\n",
        "\n",
        "def create_vocab(words):\n",
        "    # create vocabulary with indexes\n",
        "    vocab = {}\n",
        "    id = 1\n",
        "    for word in words:\n",
        "        if word not in vocab.keys():\n",
        "            vocab[word] = id\n",
        "            id += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def vectorize_data(reviews, y, vocab, LENGTH=400):\n",
        "    y = np.array([y for _ in range(len(reviews))])\n",
        "    indexed_reviews = np.zeros((len(reviews), LENGTH), dtype = np.int64)\n",
        "    for i, review in enumerate(reviews):\n",
        "        indexed_review = []\n",
        "        for word in review:\n",
        "            indexed_review.append(vocab[word])\n",
        "        indexed_reviews[i, max(LENGTH-len(review),0):] = indexed_review[:400]\n",
        "    return indexed_reviews, y\n",
        "\n",
        "def preprocessing(path1, path2, y1, y2, vocab, LENGTH=400):\n",
        "    reviews1, words1 = clean_text(path1)\n",
        "    reviews2, words2 = clean_text(path2)\n",
        "    # words1.extend(words2)\n",
        "    # print(words1)\n",
        "\n",
        "    del words1, words2\n",
        "\n",
        "    # vocab = create_vocab(words1)\n",
        "\n",
        "    x1, y1 = vectorize_data(reviews1, y1, vocab, LENGTH)\n",
        "    x2, y2 = vectorize_data(reviews2, y2, vocab, LENGTH)\n",
        "\n",
        "    x = np.concatenate((x1, x2))\n",
        "    y = np.concatenate((y1, y2))\n",
        "\n",
        "    return x, y, vocab\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "lRDL7nvqca6W"
      },
      "source": [
        "reviews, words = clean_text(\"all_merged.txt\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66iWArgxet3P"
      },
      "source": [
        "vocab = create_vocab(words)\n",
        "# vocab"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neX_8Zayfkhv"
      },
      "source": [
        "train_x, train_y, vocab = preprocessing(\"train_pos_merged.txt\", \"train_neg_merged.txt\", 0, 1, vocab)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geRNeiBuo6UP",
        "outputId": "3eedbe54-2af0-4ba2-ed27-bfd97872cdfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input = torch.from_numpy(train_x[0])\n",
        "embedding = Embedding(len(vocab), 3, padding_idx=0)\n",
        "embedding(input)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [-0.2770,  1.4500,  1.1403],\n",
              "        [-0.3521, -0.6153, -0.2365],\n",
              "        [-0.6114,  0.4785, -1.5156]], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWOwMMSXca6Y"
      },
      "source": [
        "batch_size = 100\n",
        "train_data =  TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEaZ1Ewca6Z"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x9lfzQrca6Z"
      },
      "source": [
        "class GRU_model(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, input_dim, hidden_dim, n_layers=1, LENGTH=400):\n",
        "        \n",
        "        super(GRU_model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, input_dim, padding_idx=0)\n",
        "        self.gru = GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_dim, 2)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.gru(x, h)\n",
        "        # print(f\"shape of x: {x.shape}; shape of h: {h.shape}; shape of x[:,-1]: {x[:,-1].shape}\")\n",
        "        x = self.linear(x[:,-1])\n",
        "        # print(f\"shape of x: {x.shape}\")\n",
        "        x = self.sigmoid(x)\n",
        "        return x, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZOcAlKvca6Z"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_fQL4Gca6a"
      },
      "source": [
        "def train(train_loader, vocab_size, learn_rate, input_dim=10, hidden_dim=16, EPOCHS=5):\n",
        "    \n",
        "    # Setting common hyperparameters\n",
        "    # input_dim = next(iter(train_loader))[0].shape[1]\n",
        "    # print(next(iter(train_loader))[0].shape[1])\n",
        "    output_dim = 1\n",
        "    n_layers = 1\n",
        "    # Instantiating the model\n",
        "    model = GRU_model(vocab_size, input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training\")\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.time()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            h = h.data\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out, h = model(x.to(device), h)\n",
        "            # print(f\"shape of out.squeeze(): {out.squeeze().shape}; shape of label: {label.shape}\")\n",
        "            loss = criterion(out.squeeze(), label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%100 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.time()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "def evaluate(model, test_x, test_y, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    start_time = time.clock()\n",
        "    for i in test_x.keys():\n",
        "        inp = torch.from_numpy(np.array(test_x[i]))\n",
        "        labs = torch.from_numpy(np.array(test_y[i]))\n",
        "        h = model.init_hidden(inp.shape[0])\n",
        "        out, h = model(inp.to(device).float(), h)\n",
        "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
        "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
        "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
        "    sMAPE = 0\n",
        "    for i in range(len(outputs)):\n",
        "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
        "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
        "    return outputs, targets, sMAPE"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyld89ckca6b",
        "outputId": "b9abdeb3-4f63-4fbe-8c49-13b95e103b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(\n",
        "    train_loader, \n",
        "    vocab_size = len(vocab), \n",
        "    learn_rate=0.001, \n",
        "    hidden_dim=32, \n",
        "    EPOCHS=100\n",
        "    )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training\n",
            "Epoch 1/100 Done, Total Loss: 0.6935751636823019\n",
            "Total Time Elapsed: 0.5336148738861084 seconds\n",
            "Epoch 2/100 Done, Total Loss: 0.6920966704686483\n",
            "Total Time Elapsed: 0.38527798652648926 seconds\n",
            "Epoch 3/100 Done, Total Loss: 0.6907771289348602\n",
            "Total Time Elapsed: 0.38527631759643555 seconds\n",
            "Epoch 4/100 Done, Total Loss: 0.6887146393458049\n",
            "Total Time Elapsed: 0.38058948516845703 seconds\n",
            "Epoch 5/100 Done, Total Loss: 0.6853117565313975\n",
            "Total Time Elapsed: 0.3940920829772949 seconds\n",
            "Epoch 6/100 Done, Total Loss: 0.6785733779271443\n",
            "Total Time Elapsed: 0.38701462745666504 seconds\n",
            "Epoch 7/100 Done, Total Loss: 0.6672797044118245\n",
            "Total Time Elapsed: 0.3849959373474121 seconds\n",
            "Epoch 8/100 Done, Total Loss: 0.6521039068698883\n",
            "Total Time Elapsed: 0.38364577293395996 seconds\n",
            "Epoch 9/100 Done, Total Loss: 0.6364308436711629\n",
            "Total Time Elapsed: 0.3886713981628418 seconds\n",
            "Epoch 10/100 Done, Total Loss: 0.6135675470034282\n",
            "Total Time Elapsed: 0.3943307399749756 seconds\n",
            "Epoch 11/100 Done, Total Loss: 0.6005269209543864\n",
            "Total Time Elapsed: 0.38264012336730957 seconds\n",
            "Epoch 12/100 Done, Total Loss: 0.5821572283903758\n",
            "Total Time Elapsed: 0.3883552551269531 seconds\n",
            "Epoch 13/100 Done, Total Loss: 0.5691413124402364\n",
            "Total Time Elapsed: 0.38666224479675293 seconds\n",
            "Epoch 14/100 Done, Total Loss: 0.5442131141821543\n",
            "Total Time Elapsed: 0.3831923007965088 seconds\n",
            "Epoch 15/100 Done, Total Loss: 0.537746732433637\n",
            "Total Time Elapsed: 0.3988344669342041 seconds\n",
            "Epoch 16/100 Done, Total Loss: 0.5157813747723897\n",
            "Total Time Elapsed: 0.3880167007446289 seconds\n",
            "Epoch 17/100 Done, Total Loss: 0.5016357034444809\n",
            "Total Time Elapsed: 0.38144493103027344 seconds\n",
            "Epoch 18/100 Done, Total Loss: 0.49765253663063047\n",
            "Total Time Elapsed: 0.395862340927124 seconds\n",
            "Epoch 19/100 Done, Total Loss: 0.47899844745794934\n",
            "Total Time Elapsed: 0.3875715732574463 seconds\n",
            "Epoch 20/100 Done, Total Loss: 0.4674440652132034\n",
            "Total Time Elapsed: 0.3833193778991699 seconds\n",
            "Epoch 21/100 Done, Total Loss: 0.4533156375090281\n",
            "Total Time Elapsed: 0.3968658447265625 seconds\n",
            "Epoch 22/100 Done, Total Loss: 0.4425839841365814\n",
            "Total Time Elapsed: 0.39761805534362793 seconds\n",
            "Epoch 23/100 Done, Total Loss: 0.4336497594912847\n",
            "Total Time Elapsed: 0.38597989082336426 seconds\n",
            "Epoch 24/100 Done, Total Loss: 0.42618586917718254\n",
            "Total Time Elapsed: 0.39205169677734375 seconds\n",
            "Epoch 25/100 Done, Total Loss: 0.4199633797009786\n",
            "Total Time Elapsed: 0.3919801712036133 seconds\n",
            "Epoch 26/100 Done, Total Loss: 0.41271656354268393\n",
            "Total Time Elapsed: 0.38962864875793457 seconds\n",
            "Epoch 27/100 Done, Total Loss: 0.4069032629330953\n",
            "Total Time Elapsed: 0.38732290267944336 seconds\n",
            "Epoch 28/100 Done, Total Loss: 0.40477588375409446\n",
            "Total Time Elapsed: 0.3973410129547119 seconds\n",
            "Epoch 29/100 Done, Total Loss: 0.40073262055714926\n",
            "Total Time Elapsed: 0.39077305793762207 seconds\n",
            "Epoch 30/100 Done, Total Loss: 0.39542956749598185\n",
            "Total Time Elapsed: 0.390209436416626 seconds\n",
            "Epoch 31/100 Done, Total Loss: 0.390289181470871\n",
            "Total Time Elapsed: 0.38583874702453613 seconds\n",
            "Epoch 32/100 Done, Total Loss: 0.3891450673341751\n",
            "Total Time Elapsed: 0.38327789306640625 seconds\n",
            "Epoch 33/100 Done, Total Loss: 0.3848407169183095\n",
            "Total Time Elapsed: 0.37908196449279785 seconds\n",
            "Epoch 34/100 Done, Total Loss: 0.38310414950052896\n",
            "Total Time Elapsed: 0.3908214569091797 seconds\n",
            "Epoch 35/100 Done, Total Loss: 0.3847817371288935\n",
            "Total Time Elapsed: 0.38245272636413574 seconds\n",
            "Epoch 36/100 Done, Total Loss: 0.3790191312630971\n",
            "Total Time Elapsed: 0.3869318962097168 seconds\n",
            "Epoch 37/100 Done, Total Loss: 0.37696262697378796\n",
            "Total Time Elapsed: 0.380983829498291 seconds\n",
            "Epoch 38/100 Done, Total Loss: 0.3766436586777369\n",
            "Total Time Elapsed: 0.38414812088012695 seconds\n",
            "Epoch 39/100 Done, Total Loss: 0.3739620993534724\n",
            "Total Time Elapsed: 0.3930361270904541 seconds\n",
            "Epoch 40/100 Done, Total Loss: 0.3761404246091843\n",
            "Total Time Elapsed: 0.3899812698364258 seconds\n",
            "Epoch 41/100 Done, Total Loss: 0.3732944349447886\n",
            "Total Time Elapsed: 0.3909025192260742 seconds\n",
            "Epoch 42/100 Done, Total Loss: 0.37004462679227196\n",
            "Total Time Elapsed: 0.38403773307800293 seconds\n",
            "Epoch 43/100 Done, Total Loss: 0.3688062975804011\n",
            "Total Time Elapsed: 0.38127660751342773 seconds\n",
            "Epoch 44/100 Done, Total Loss: 0.3677432378133138\n",
            "Total Time Elapsed: 0.4009103775024414 seconds\n",
            "Epoch 45/100 Done, Total Loss: 0.3674473236004511\n",
            "Total Time Elapsed: 0.38468265533447266 seconds\n",
            "Epoch 46/100 Done, Total Loss: 0.36552194754282635\n",
            "Total Time Elapsed: 0.39722728729248047 seconds\n",
            "Epoch 47/100 Done, Total Loss: 0.36462106704711916\n",
            "Total Time Elapsed: 0.39502739906311035 seconds\n",
            "Epoch 48/100 Done, Total Loss: 0.3637306660413742\n",
            "Total Time Elapsed: 0.40082216262817383 seconds\n",
            "Epoch 49/100 Done, Total Loss: 0.3629978666702906\n",
            "Total Time Elapsed: 0.39108729362487793 seconds\n",
            "Epoch 50/100 Done, Total Loss: 0.36267962753772737\n",
            "Total Time Elapsed: 0.38790464401245117 seconds\n",
            "Epoch 51/100 Done, Total Loss: 0.3612343162298203\n",
            "Total Time Elapsed: 0.3991057872772217 seconds\n",
            "Epoch 52/100 Done, Total Loss: 0.36049152811368307\n",
            "Total Time Elapsed: 0.4189422130584717 seconds\n",
            "Epoch 53/100 Done, Total Loss: 0.35984135965506236\n",
            "Total Time Elapsed: 0.4015772342681885 seconds\n",
            "Epoch 54/100 Done, Total Loss: 0.35893650352954865\n",
            "Total Time Elapsed: 0.39916276931762695 seconds\n",
            "Epoch 55/100 Done, Total Loss: 0.3582956810792287\n",
            "Total Time Elapsed: 0.4024806022644043 seconds\n",
            "Epoch 56/100 Done, Total Loss: 0.35737160245577493\n",
            "Total Time Elapsed: 0.38825249671936035 seconds\n",
            "Epoch 57/100 Done, Total Loss: 0.35711382428805033\n",
            "Total Time Elapsed: 0.39812421798706055 seconds\n",
            "Epoch 58/100 Done, Total Loss: 0.35700812339782717\n",
            "Total Time Elapsed: 0.38849425315856934 seconds\n",
            "Epoch 59/100 Done, Total Loss: 0.3570393204689026\n",
            "Total Time Elapsed: 0.3939650058746338 seconds\n",
            "Epoch 60/100 Done, Total Loss: 0.35654649237791697\n",
            "Total Time Elapsed: 0.3883495330810547 seconds\n",
            "Epoch 61/100 Done, Total Loss: 0.35647728939851125\n",
            "Total Time Elapsed: 0.3901686668395996 seconds\n",
            "Epoch 62/100 Done, Total Loss: 0.3564401080211004\n",
            "Total Time Elapsed: 0.38875293731689453 seconds\n",
            "Epoch 63/100 Done, Total Loss: 0.3566265861193339\n",
            "Total Time Elapsed: 0.40102076530456543 seconds\n",
            "Epoch 64/100 Done, Total Loss: 0.3708958178758621\n",
            "Total Time Elapsed: 0.40630316734313965 seconds\n",
            "Epoch 65/100 Done, Total Loss: 0.3668367266654968\n",
            "Total Time Elapsed: 0.397432804107666 seconds\n",
            "Epoch 66/100 Done, Total Loss: 0.3677085091670354\n",
            "Total Time Elapsed: 0.3852813243865967 seconds\n",
            "Epoch 67/100 Done, Total Loss: 0.3861030687888463\n",
            "Total Time Elapsed: 0.40276384353637695 seconds\n",
            "Epoch 68/100 Done, Total Loss: 0.35653457542260486\n",
            "Total Time Elapsed: 0.386594295501709 seconds\n",
            "Epoch 69/100 Done, Total Loss: 0.35511515140533445\n",
            "Total Time Elapsed: 0.38382887840270996 seconds\n",
            "Epoch 70/100 Done, Total Loss: 0.35464539428551994\n",
            "Total Time Elapsed: 0.3951287269592285 seconds\n",
            "Epoch 71/100 Done, Total Loss: 0.3543713043133418\n",
            "Total Time Elapsed: 0.38945508003234863 seconds\n",
            "Epoch 72/100 Done, Total Loss: 0.3542255262533824\n",
            "Total Time Elapsed: 0.3983466625213623 seconds\n",
            "Epoch 73/100 Done, Total Loss: 0.354132616519928\n",
            "Total Time Elapsed: 0.3877131938934326 seconds\n",
            "Epoch 74/100 Done, Total Loss: 0.3540661334991455\n",
            "Total Time Elapsed: 0.39510035514831543 seconds\n",
            "Epoch 75/100 Done, Total Loss: 0.3540114263693492\n",
            "Total Time Elapsed: 0.38895630836486816 seconds\n",
            "Epoch 76/100 Done, Total Loss: 0.3539691150188446\n",
            "Total Time Elapsed: 0.4017970561981201 seconds\n",
            "Epoch 77/100 Done, Total Loss: 0.35391055842240654\n",
            "Total Time Elapsed: 0.3905651569366455 seconds\n",
            "Epoch 78/100 Done, Total Loss: 0.3537430187066396\n",
            "Total Time Elapsed: 0.40917372703552246 seconds\n",
            "Epoch 79/100 Done, Total Loss: 0.35335627098878225\n",
            "Total Time Elapsed: 0.38700008392333984 seconds\n",
            "Epoch 80/100 Done, Total Loss: 0.3532755086819331\n",
            "Total Time Elapsed: 0.39426112174987793 seconds\n",
            "Epoch 81/100 Done, Total Loss: 0.3532087723414103\n",
            "Total Time Elapsed: 0.3983767032623291 seconds\n",
            "Epoch 82/100 Done, Total Loss: 0.35303528904914855\n",
            "Total Time Elapsed: 0.3957641124725342 seconds\n",
            "Epoch 83/100 Done, Total Loss: 0.35296566486358644\n",
            "Total Time Elapsed: 0.3828151226043701 seconds\n",
            "Epoch 84/100 Done, Total Loss: 0.3525412579377492\n",
            "Total Time Elapsed: 0.38793396949768066 seconds\n",
            "Epoch 85/100 Done, Total Loss: 0.35250916679700217\n",
            "Total Time Elapsed: 0.39315295219421387 seconds\n",
            "Epoch 86/100 Done, Total Loss: 0.3524695585171382\n",
            "Total Time Elapsed: 0.40219783782958984 seconds\n",
            "Epoch 87/100 Done, Total Loss: 0.35249464213848114\n",
            "Total Time Elapsed: 0.3899099826812744 seconds\n",
            "Epoch 88/100 Done, Total Loss: 0.35248035192489624\n",
            "Total Time Elapsed: 0.392200231552124 seconds\n",
            "Epoch 89/100 Done, Total Loss: 0.3524702082077662\n",
            "Total Time Elapsed: 0.380662202835083 seconds\n",
            "Epoch 90/100 Done, Total Loss: 0.3524123777945836\n",
            "Total Time Elapsed: 0.39935755729675293 seconds\n",
            "Epoch 91/100 Done, Total Loss: 0.3742141236861547\n",
            "Total Time Elapsed: 0.3830108642578125 seconds\n",
            "Epoch 92/100 Done, Total Loss: 0.5314998428026835\n",
            "Total Time Elapsed: 0.3934776782989502 seconds\n",
            "Epoch 93/100 Done, Total Loss: 0.38916612168153125\n",
            "Total Time Elapsed: 0.39632081985473633 seconds\n",
            "Epoch 94/100 Done, Total Loss: 0.364983210961024\n",
            "Total Time Elapsed: 0.3884563446044922 seconds\n",
            "Epoch 95/100 Done, Total Loss: 0.3582126826047897\n",
            "Total Time Elapsed: 0.3953726291656494 seconds\n",
            "Epoch 96/100 Done, Total Loss: 0.35379053354263307\n",
            "Total Time Elapsed: 0.38646602630615234 seconds\n",
            "Epoch 97/100 Done, Total Loss: 0.3547835320234299\n",
            "Total Time Elapsed: 0.3873932361602783 seconds\n",
            "Epoch 98/100 Done, Total Loss: 0.36480091909567514\n",
            "Total Time Elapsed: 0.39209651947021484 seconds\n",
            "Epoch 99/100 Done, Total Loss: 0.38372051119804385\n",
            "Total Time Elapsed: 0.3965928554534912 seconds\n",
            "Epoch 100/100 Done, Total Loss: 0.3540740221738815\n",
            "Total Time Elapsed: 0.38877081871032715 seconds\n",
            "Total Training Time: 39.27203869819641 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU_model(\n",
              "  (embedding): Embedding(39237, 10, padding_idx=0)\n",
              "  (gru): GRU(10, 32, batch_first=True)\n",
              "  (linear): Linear(in_features=32, out_features=2, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu30OGJlca6b",
        "outputId": "fdd577a1-3c05-4ba1-9262-8233a677afb7"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}