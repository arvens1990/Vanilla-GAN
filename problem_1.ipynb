{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('HW3': virtualenvwrapper)"
    },
    "metadata": {
      "interpreter": {
        "hash": "a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65"
      }
    },
    "colab": {
      "name": "Copy of problem_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb3v-7vPca6E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import Module, GRU, Embedding, Linear, Sigmoid, CrossEntropyLoss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2vJl0i5ca6V"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHjwy_3zcpui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000bd502-69fa-4ec4-f6c8-d43b4b00adc5"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrqY84fca6V"
      },
      "source": [
        "\"data/sentiment_analysis/train_pos_merged.txt\"\n",
        "# function clearing HTML tags from text\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "# preprocessing\n",
        "def clean_text(path):\n",
        "    reviews = []\n",
        "    all_words = []\n",
        "    with open(path) as pos:\n",
        "        lines = pos.readlines()\n",
        "        for line in lines:\n",
        "            #clear html tags\n",
        "            line = cleanhtml(line)\n",
        "            # lower case and punctuation\n",
        "            line = re.sub(r'[^a-zA-Z]', ' ', line.lower())\n",
        "            # split to list of words\n",
        "            words = line.split()\n",
        "            # add list to reviews\n",
        "            reviews.append(words)\n",
        "            # extend words with new review\n",
        "            all_words.extend(words)\n",
        "\n",
        "    return reviews, all_words\n",
        "\n",
        "def create_vocab(words):\n",
        "    # create vocabulary with indexes\n",
        "    vocab = {}\n",
        "    id = 1\n",
        "    for word in words:\n",
        "        if word not in vocab.keys():\n",
        "            vocab[word] = id\n",
        "            id += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def vectorize_data(reviews, y, vocab, LENGTH=400):\n",
        "    y = np.array([y for _ in range(len(reviews))])\n",
        "    indexed_reviews = np.zeros((len(reviews), LENGTH), dtype = np.int64)\n",
        "    for i, review in enumerate(reviews):\n",
        "        indexed_review = []\n",
        "        for word in review:\n",
        "            indexed_review.append(vocab[word])\n",
        "        indexed_reviews[i, max(LENGTH-len(review),0):] = indexed_review[:400]\n",
        "    return indexed_reviews, y\n",
        "\n",
        "def preprocessing(path1, path2, y1, y2, vocab, LENGTH=400):\n",
        "    reviews1, words1 = clean_text(path1)\n",
        "    reviews2, words2 = clean_text(path2)\n",
        "    # words1.extend(words2)\n",
        "    # print(words1)\n",
        "\n",
        "    del words1, words2\n",
        "\n",
        "    # vocab = create_vocab(words1)\n",
        "\n",
        "    x1, y1 = vectorize_data(reviews1, y1, vocab, LENGTH)\n",
        "    x2, y2 = vectorize_data(reviews2, y2, vocab, LENGTH)\n",
        "\n",
        "    x = np.concatenate((x1, x2))\n",
        "    y = np.concatenate((y1, y2))\n",
        "\n",
        "    return x, y #, vocab\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "lRDL7nvqca6W"
      },
      "source": [
        "# !ls /content/drive\n",
        "# path = \"/content/drive/MyDrive/Deep_Learning/sentiment_analysis/\"\n",
        "# in \"./data/sentiment_analysis/\" we have 5 files: positive/negative train/test (4) + all merged\n",
        "path = \"./data/sentiment_analysis/\"\n",
        "# we use all_merged to build vocab\n",
        "reviews, words = clean_text(path + \"all_merged.txt\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66iWArgxet3P"
      },
      "source": [
        "vocab = create_vocab(words)\n",
        "# vocab"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neX_8Zayfkhv"
      },
      "source": [
        "train_x, train_y = preprocessing(path + \"train_pos_merged.txt\", path + \"train_neg_merged.txt\", 0, 1, vocab)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geRNeiBuo6UP"
      },
      "source": [
        "# input = torch.from_numpy(train_x[0])\n",
        "# embedding = Embedding(len(vocab), 3, padding_idx=0)\n",
        "# embedding(input)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWOwMMSXca6Y"
      },
      "source": [
        "batch_size = 128\n",
        "train_data =  TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEaZ1Ewca6Z"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x9lfzQrca6Z"
      },
      "source": [
        "class GRU_model(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, input_dim, hidden_dim, n_layers=1, LENGTH=400):\n",
        "        \n",
        "        super(GRU_model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, input_dim, padding_idx=0)\n",
        "        self.gru = GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_dim, 2)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        # print(f\"shape of x: {x.shape}; shape of h: {h.shape}; shape of x[:,-1]: {x[:,-1].shape}\")\n",
        "        x, h = self.gru(x, h)\n",
        "        # print(f\"shape of x: {x.shape}; shape of h: {h.shape}; shape of x[:,-1]: {x[:,-1].shape}\")\n",
        "        x = self.linear(x[:,-1])\n",
        "        # print(f\"shape of x: {x.shape}\")\n",
        "        x = self.sigmoid(x)\n",
        "        return x, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZOcAlKvca6Z"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_fQL4Gca6a"
      },
      "source": [
        "losses = []\n",
        "model = GRU_model(len(vocab), 1, 32, 1, 1)\n",
        "def gru_train(train_loader, vocab_size, learn_rate, input_dim=10, hidden_dim=16, EPOCHS=5, schedule_mult=0.99):\n",
        "    \n",
        "    \n",
        "    output_dim = 1\n",
        "    n_layers = 1\n",
        "    # Instantiating the model\n",
        "    # model = GRU_model(vocab_size, input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "    # losses = []\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    lmbda = lambda epoch: schedule_mult\n",
        "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Starting Training\")\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.time()\n",
        "        # h = model.init_hidden(train_loader.batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            h = model.init_hidden(x.shape[0])\n",
        "            h = h.data\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out, h = model(x.to(device), h)\n",
        "            # print(f\"shape of out.squeeze(): {out.squeeze().shape}; shape of label: {label.shape}\")\n",
        "            loss = criterion(out.squeeze(), label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            losses.append(avg_loss/counter)\n",
        "            if counter%100 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.time()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "        scheduler.step()\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model, losses\n",
        "\n",
        "\n",
        "def gru_evaluate(model, test_loader): \n",
        "    outputs = []\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    err = 0\n",
        "    for x, label in test_loader:\n",
        "        h = model.init_hidden(x.shape[0]).data\n",
        "        input = x.to(device)\n",
        "        output, h_out = model(input, h)\n",
        "        result = torch.argmax(output, dim=1)\n",
        "        results.append(result)\n",
        "        err += torch.abs(result.to(device) - label.to(device)).sum()\n",
        "    accuracy = 1 - err/len(test_loader)\n",
        "    return accuracy, outputs, results"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "learn_rate=0.1\n",
        "hidden_dim=32 \n",
        "EPOCHS=15\n",
        "schedule_mult=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyld89ckca6b",
        "tags": []
      },
      "source": [
        "gru_model, gru_losses = gru_train(\n",
        "    train_loader, \n",
        "    vocab_size = vocab_size, \n",
        "    learn_rate=learn_rate, \n",
        "    hidden_dim=hidden_dim, \n",
        "    EPOCHS=EPOCHS,\n",
        "    schedule_mult=schedule_mult\n",
        "    )"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
        "torch.save(gru_model.state_dict(), \"./models/gru/gru_model\" + dt_string + \".pt\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEjXCRTp6U3W"
      },
      "source": [
        "test_x, test_y = preprocessing(path + \"test_pos_merged.txt\", path + \"test_neg_merged.txt\", 0, 1, vocab)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC0Zdgdj7Lz5"
      },
      "source": [
        "batch_size = 128\n",
        "test_data =  TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "test_loader = DataLoader(test_data) #, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# gru_model = GRU_model(len(vocab), 10, 32, 1, 1)\n",
        "# gru_model.load_state_dict(torch.load(\"./models/gru/gru_rnn.pt\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIJ2sIbz7hGH"
      },
      "source": [
        "test_accuracy, test_outputs, test_results = gru_evaluate(gru_model, test_loader)\n",
        "# train_accuracy, train_outputs, train_results = gru_evaluate(gru_model, train_loader)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O08JCDG7Ct4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee858c9-c457-49f9-dff0-ec721344eccc"
      },
      "source": [
        "print(f\"\"\"train_accuracy={train_accuracy}\n",
        "test_accuracy={test_accuracy}\"\"\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_accuracy=-5.541666507720947\ntest_accuracy=0.6836667060852051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRS6VyanwsKT"
      },
      "source": [
        "## **Part 3** MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQs5x3q7wq_q"
      },
      "source": [
        "class MLP_model(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, input_dim, hidden_dim, LENGTH = 400):\n",
        "        \n",
        "        super(MLP_model, self).__init__()\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, input_dim, padding_idx=0)\n",
        "        self.fc1 = Linear(input_dim*LENGTH, hidden_dim)\n",
        "        self.fc2 = Linear(hidden_dim, 2)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVLN3-9xo5w"
      },
      "source": [
        "model = MLP_model(len(vocab), 10, 100)\n",
        "losses = []\n",
        "def mlp_train(train_loader, vocab_size, learn_rate, input_dim=10, hidden_dim=16, EPOCHS=5):\n",
        "    \n",
        "    # Instantiating the model\n",
        "    # model = MLP_model(vocab_size, input_dim, hidden_dim)\n",
        "    # print(model)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "\n",
        "    lmbda = lambda epoch: 0.95\n",
        "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training\")\n",
        "    epoch_times = []\n",
        "    # losses = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.time()\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out = model(x.to(device))\n",
        "            # print(out.shape)\n",
        "            # print(f\"shape of out: {out.shape}; shape of label: {label.shape}\")\n",
        "            # return 0\n",
        "            loss = criterion(out, label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            losses.append(avg_loss/counter)\n",
        "            if counter%100 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.time()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        # print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "        scheduler.step()\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model, losses\n",
        "\n",
        "\n",
        "def mlp_evaluate(model, test_loader): \n",
        "    outputs = []\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    err = 0\n",
        "    for x, label in test_loader:\n",
        "        input = x.to(device)\n",
        "        output = model(input)\n",
        "        result = torch.argmax(output, dim=1)\n",
        "        results.append(result)\n",
        "        outputs.append(output)\n",
        "        err += torch.abs(result.to(device) - label.to(device)).sum()\n",
        "    accuracy = 1 - err/len(test_loader)\n",
        "    return accuracy, outputs, results"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg0KFulFyPiQ",
        "outputId": "844c7c20-4e91-47b0-e4f5-a345a2ea7f1e",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "mlp_model, mlp_losses = mlp_train(train_loader, len(vocab), 0.01, hidden_dim=100 ,EPOCHS=20)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "erage Loss for Epoch: 0.9654355515604434\n",
            "Epoch 12......Step: 2400/3000....... Average Loss for Epoch: 0.9382616393268108\n",
            "Epoch 12......Step: 2500/3000....... Average Loss for Epoch: 0.9132616400718689\n",
            "Epoch 12......Step: 2600/3000....... Average Loss for Epoch: 0.8901847176826917\n",
            "Epoch 12......Step: 2700/3000....... Average Loss for Epoch: 0.8688171969519721\n",
            "Epoch 12......Step: 2800/3000....... Average Loss for Epoch: 0.8489759277020182\n",
            "Epoch 12......Step: 2900/3000....... Average Loss for Epoch: 0.8305030218486128\n",
            "Epoch 12......Step: 3000/3000....... Average Loss for Epoch: 0.8132616430521011\n",
            "Epoch 12/20 Done, Total Loss: 0.8132616430521011\n",
            "Epoch 13......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 13......Step: 1600/3000....... Average Loss for Epoch: 1.250761630013585\n",
            "Epoch 13......Step: 1700/3000....... Average Loss for Epoch: 1.195614572833566\n",
            "Epoch 13......Step: 1800/3000....... Average Loss for Epoch: 1.146594966451327\n",
            "Epoch 13......Step: 1900/3000....... Average Loss for Epoch: 1.1027353186356394\n",
            "Epoch 13......Step: 2000/3000....... Average Loss for Epoch: 1.0632616356015205\n",
            "Epoch 13......Step: 2100/3000....... Average Loss for Epoch: 1.0275473509516035\n",
            "Epoch 13......Step: 2200/3000....... Average Loss for Epoch: 0.9950798194516789\n",
            "Epoch 13......Step: 2300/3000....... Average Loss for Epoch: 0.9654355515604434\n",
            "Epoch 13......Step: 2400/3000....... Average Loss for Epoch: 0.9382616393268108\n",
            "Epoch 13......Step: 2500/3000....... Average Loss for Epoch: 0.9132616400718689\n",
            "Epoch 13......Step: 2600/3000....... Average Loss for Epoch: 0.8901847176826917\n",
            "Epoch 13......Step: 2700/3000....... Average Loss for Epoch: 0.8688171969519721\n",
            "Epoch 13......Step: 2800/3000....... Average Loss for Epoch: 0.8489759277020182\n",
            "Epoch 13......Step: 2900/3000....... Average Loss for Epoch: 0.8305030218486128\n",
            "Epoch 13......Step: 3000/3000....... Average Loss for Epoch: 0.8132616430521011\n",
            "Epoch 13/20 Done, Total Loss: 0.8132616430521011\n",
            "Epoch 14......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 14......Step: 1600/3000....... Average Loss for Epoch: 1.250761630013585\n",
            "Epoch 14......Step: 1700/3000....... Average Loss for Epoch: 1.195614572833566\n",
            "Epoch 14......Step: 1800/3000....... Average Loss for Epoch: 1.146594966451327\n",
            "Epoch 14......Step: 1900/3000....... Average Loss for Epoch: 1.1027353186356394\n",
            "Epoch 14......Step: 2000/3000....... Average Loss for Epoch: 1.0632616356015205\n",
            "Epoch 14......Step: 2100/3000....... Average Loss for Epoch: 1.0275473509516035\n",
            "Epoch 14......Step: 2200/3000....... Average Loss for Epoch: 0.9950798194516789\n",
            "Epoch 14......Step: 2300/3000....... Average Loss for Epoch: 0.9654355515604434\n",
            "Epoch 14......Step: 2400/3000....... Average Loss for Epoch: 0.9382616393268108\n",
            "Epoch 14......Step: 2500/3000....... Average Loss for Epoch: 0.9132616400718689\n",
            "Epoch 14......Step: 2600/3000....... Average Loss for Epoch: 0.8901847176826917\n",
            "Epoch 14......Step: 2700/3000....... Average Loss for Epoch: 0.8688171969519721\n",
            "Epoch 14......Step: 2800/3000....... Average Loss for Epoch: 0.8489759277020182\n",
            "Epoch 14......Step: 2900/3000....... Average Loss for Epoch: 0.8305030218486128\n",
            "Epoch 14......Step: 3000/3000....... Average Loss for Epoch: 0.8132616430521011\n",
            "Epoch 14/20 Done, Total Loss: 0.8132616430521011\n",
            "Epoch 15......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 15......Step: 1600/3000....... Average Loss for Epoch: 1.250761630013585\n",
            "Epoch 15......Step: 1700/3000....... Average Loss for Epoch: 1.195614572833566\n",
            "Epoch 15......Step: 1800/3000....... Average Loss for Epoch: 1.146594966451327\n",
            "Epoch 15......Step: 1900/3000....... Average Loss for Epoch: 1.1027353186356394\n",
            "Epoch 15......Step: 2000/3000....... Average Loss for Epoch: 1.0632616356015205\n",
            "Epoch 15......Step: 2100/3000....... Average Loss for Epoch: 1.0275473509516035\n",
            "Epoch 15......Step: 2200/3000....... Average Loss for Epoch: 0.9950798194516789\n",
            "Epoch 15......Step: 2300/3000....... Average Loss for Epoch: 0.9654355515604434\n",
            "Epoch 15......Step: 2400/3000....... Average Loss for Epoch: 0.9382616393268108\n",
            "Epoch 15......Step: 2500/3000....... Average Loss for Epoch: 0.9132616400718689\n",
            "Epoch 15......Step: 2600/3000....... Average Loss for Epoch: 0.8901847176826917\n",
            "Epoch 15......Step: 2700/3000....... Average Loss for Epoch: 0.8688171969519721\n",
            "Epoch 15......Step: 2800/3000....... Average Loss for Epoch: 0.8489759277020182\n",
            "Epoch 15......Step: 2900/3000....... Average Loss for Epoch: 0.8305030218486128\n",
            "Epoch 15......Step: 3000/3000....... Average Loss for Epoch: 0.8132616430521011\n",
            "Epoch 15/20 Done, Total Loss: 0.8132616430521011\n",
            "Epoch 16......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 16......Step: 1600/3000....... Average Loss for Epoch: 1.250761630013585\n",
            "Epoch 16......Step: 1700/3000....... Average Loss for Epoch: 1.195614572833566\n",
            "Epoch 16......Step: 1800/3000....... Average Loss for Epoch: 1.146594966451327\n",
            "Epoch 16......Step: 1900/3000....... Average Loss for Epoch: 1.1027353186356394\n",
            "Epoch 16......Step: 2000/3000....... Average Loss for Epoch: 1.0632616356015205\n",
            "Epoch 16......Step: 2100/3000....... Average Loss for Epoch: 1.0275473509516035\n",
            "Epoch 16......Step: 2200/3000....... Average Loss for Epoch: 0.9950798194516789\n",
            "Epoch 16......Step: 2300/3000....... Average Loss for Epoch: 0.9654355515604434\n",
            "Epoch 16......Step: 2400/3000....... Average Loss for Epoch: 0.9382616393268108\n",
            "Epoch 16......Step: 2500/3000....... Average Loss for Epoch: 0.9132616400718689\n",
            "Epoch 16......Step: 2600/3000....... Average Loss for Epoch: 0.8901847176826917\n",
            "Epoch 16......Step: 2700/3000....... Average Loss for Epoch: 0.8688171969519721\n",
            "Epoch 16......Step: 2800/3000....... Average Loss for Epoch: 0.8489759277020182\n",
            "Epoch 16......Step: 2900/3000....... Average Loss for Epoch: 0.8305030218486128\n",
            "Epoch 16......Step: 3000/3000....... Average Loss for Epoch: 0.8132616430521011\n",
            "Epoch 16/20 Done, Total Loss: 0.8132616430521011\n",
            "Epoch 17......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 17......Step: 1300/3000....... Average Loss for Epoch: 1.3127846170388735\n",
            "Epoch 17......Step: 1400/3000....... Average Loss for Epoch: 1.312818689261164\n",
            "Epoch 17......Step: 1500/3000....... Average Loss for Epoch: 1.3128482185204824\n",
            "Epoch 17......Step: 1600/3000....... Average Loss for Epoch: 1.2503740584850311\n",
            "Epoch 17......Step: 1700/3000....... Average Loss for Epoch: 1.195249924186398\n",
            "Epoch 17......Step: 1800/3000....... Average Loss for Epoch: 1.146250576062335\n",
            "Epoch 17......Step: 1900/3000....... Average Loss for Epoch: 1.1024090540565943\n",
            "Epoch 17......Step: 2000/3000....... Average Loss for Epoch: 1.0629516842514277\n",
            "Epoch 17......Step: 2100/3000....... Average Loss for Epoch: 1.0272521591896102\n",
            "Epoch 17......Step: 2200/3000....... Average Loss for Epoch: 0.994798045497049\n",
            "Epoch 17......Step: 2300/3000....... Average Loss for Epoch: 0.9651660286473192\n",
            "Epoch 17......Step: 2400/3000....... Average Loss for Epoch: 0.9380033465350668\n",
            "Epoch 17......Step: 2500/3000....... Average Loss for Epoch: 0.9130136789917946\n",
            "Epoch 17......Step: 2600/3000....... Average Loss for Epoch: 0.8899462935672356\n",
            "Epoch 17......Step: 2700/3000....... Average Loss for Epoch: 0.8685876033593107\n",
            "Epoch 17......Step: 2800/3000....... Average Loss for Epoch: 0.8487545338805232\n",
            "Epoch 17......Step: 2900/3000....... Average Loss for Epoch: 0.8302892622968246\n",
            "Epoch 17......Step: 3000/3000....... Average Loss for Epoch: 0.8130550088187058\n",
            "Epoch 17/20 Done, Total Loss: 0.8130550088187058\n",
            "Epoch 18......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 1100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 1200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 18......Step: 1300/3000....... Average Loss for Epoch: 1.3127857664456735\n",
            "Epoch 18......Step: 1400/3000....... Average Loss for Epoch: 1.3128197565674782\n",
            "Epoch 18......Step: 1500/3000....... Average Loss for Epoch: 1.3124358050425848\n",
            "Epoch 18......Step: 1600/3000....... Average Loss for Epoch: 1.249987420849502\n",
            "Epoch 18......Step: 1700/3000....... Average Loss for Epoch: 1.1948859053850174\n",
            "Epoch 18......Step: 1800/3000....... Average Loss for Epoch: 1.1459067805276977\n",
            "Epoch 18......Step: 1900/3000....... Average Loss for Epoch: 1.1020842163656888\n",
            "Epoch 18......Step: 2000/3000....... Average Loss for Epoch: 1.0626430884450675\n",
            "Epoch 18......Step: 2100/3000....... Average Loss for Epoch: 1.0269582584216481\n",
            "Epoch 18......Step: 2200/3000....... Average Loss for Epoch: 0.9945175038549033\n",
            "Epoch 18......Step: 2300/3000....... Average Loss for Epoch: 0.9648976844678755\n",
            "Epoch 18......Step: 2400/3000....... Average Loss for Epoch: 0.9377461833630999\n",
            "Epoch 18......Step: 2500/3000....... Average Loss for Epoch: 0.9127668023467064\n",
            "Epoch 18......Step: 2600/3000....... Average Loss for Epoch: 0.8897089121777277\n",
            "Epoch 18......Step: 2700/3000....... Average Loss for Epoch: 0.868359013873118\n",
            "Epoch 18......Step: 2800/3000....... Average Loss for Epoch: 0.8485341083045517\n",
            "Epoch 18......Step: 2900/3000....... Average Loss for Epoch: 0.830076437602783\n",
            "Epoch 18......Step: 3000/3000....... Average Loss for Epoch: 0.8128492782811324\n",
            "Epoch 18/20 Done, Total Loss: 0.8128492782811324\n",
            "Epoch 19......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 200/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 300/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 400/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 500/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 600/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 700/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 800/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 900/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 1000/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 19......Step: 1100/3000....... Average Loss for Epoch: 1.3132612659714438\n",
            "Epoch 19......Step: 1200/3000....... Average Loss for Epoch: 1.3132612916827202\n",
            "Epoch 19......Step: 1300/3000....... Average Loss for Epoch: 1.2923594804681264\n",
            "Epoch 19......Step: 1400/3000....... Average Loss for Epoch: 1.2655045463570527\n",
            "Epoch 19......Step: 1500/3000....... Average Loss for Epoch: 1.2280039079586664\n",
            "Epoch 19......Step: 1600/3000....... Average Loss for Epoch: 1.1929449659585953\n",
            "Epoch 19......Step: 1700/3000....... Average Loss for Epoch: 1.1617573997553656\n",
            "Epoch 19......Step: 1800/3000....... Average Loss for Epoch: 1.1329944861431918\n",
            "Epoch 19......Step: 1900/3000....... Average Loss for Epoch: 1.0957635113596915\n",
            "Epoch 19......Step: 2000/3000....... Average Loss for Epoch: 1.0623344150036573\n",
            "Epoch 19......Step: 2100/3000....... Average Loss for Epoch: 1.0306440372410275\n",
            "Epoch 19......Step: 2200/3000....... Average Loss for Epoch: 1.0016609342802654\n",
            "Epoch 19......Step: 2300/3000....... Average Loss for Epoch: 0.9748297809777052\n",
            "Epoch 19......Step: 2400/3000....... Average Loss for Epoch: 0.948688126069804\n",
            "Epoch 19......Step: 2500/3000....... Average Loss for Epoch: 0.923726929974556\n",
            "Epoch 19......Step: 2600/3000....... Average Loss for Epoch: 0.9015624847893532\n",
            "Epoch 19......Step: 2700/3000....... Average Loss for Epoch: 0.8805093243938905\n",
            "Epoch 19......Step: 2800/3000....... Average Loss for Epoch: 0.8603861525654792\n",
            "Epoch 19......Step: 2900/3000....... Average Loss for Epoch: 0.841912775707656\n",
            "Epoch 19......Step: 3000/3000....... Average Loss for Epoch: 0.8246137188076973\n",
            "Epoch 19/20 Done, Total Loss: 0.8246137188076973\n",
            "Epoch 20......Step: 100/3000....... Average Loss for Epoch: 1.31326162815094\n",
            "Epoch 20......Step: 200/3000....... Average Loss for Epoch: 1.3101610559225083\n",
            "Epoch 20......Step: 300/3000....... Average Loss for Epoch: 1.3091275318463644\n",
            "Epoch 20......Step: 400/3000....... Average Loss for Epoch: 1.3101610559225083\n",
            "Epoch 20......Step: 500/3000....... Average Loss for Epoch: 1.3107811703681946\n",
            "Epoch 20......Step: 600/3000....... Average Loss for Epoch: 1.311194575826327\n",
            "Epoch 20......Step: 700/3000....... Average Loss for Epoch: 1.3106039952380317\n",
            "Epoch 20......Step: 800/3000....... Average Loss for Epoch: 1.309386039674282\n",
            "Epoch 20......Step: 900/3000....... Average Loss for Epoch: 1.309815036588245\n",
            "Epoch 20......Step: 1000/3000....... Average Loss for Epoch: 1.3101596957445145\n",
            "Epoch 20......Step: 1100/3000....... Average Loss for Epoch: 1.3098779491944748\n",
            "Epoch 20......Step: 1200/3000....... Average Loss for Epoch: 1.3086096363266309\n",
            "Epoch 20......Step: 1300/3000....... Average Loss for Epoch: 1.3075364485153784\n",
            "Epoch 20......Step: 1400/3000....... Average Loss for Epoch: 1.3070595121383668\n",
            "Epoch 20......Step: 1500/3000....... Average Loss for Epoch: 1.3066461672782899\n",
            "Epoch 20......Step: 1600/3000....... Average Loss for Epoch: 1.2445596354454755\n",
            "Epoch 20......Step: 1700/3000....... Average Loss for Epoch: 1.1900008635485873\n",
            "Epoch 20......Step: 1800/3000....... Average Loss for Epoch: 1.1412931299044027\n",
            "Epoch 20......Step: 1900/3000....... Average Loss for Epoch: 1.0977125261175005\n",
            "Epoch 20......Step: 2000/3000....... Average Loss for Epoch: 1.0588698660284281\n",
            "Epoch 20......Step: 2100/3000....... Average Loss for Epoch: 1.023364713262944\n",
            "Epoch 20......Step: 2200/3000....... Average Loss for Epoch: 0.9914326510510661\n",
            "Epoch 20......Step: 2300/3000....... Average Loss for Epoch: 0.9619469556989877\n",
            "Epoch 20......Step: 2400/3000....... Average Loss for Epoch: 0.9349184016262492\n",
            "Epoch 20......Step: 2500/3000....... Average Loss for Epoch: 0.9100521318793297\n",
            "Epoch 20......Step: 2600/3000....... Average Loss for Epoch: 0.8873908718732687\n",
            "Epoch 20......Step: 2700/3000....... Average Loss for Epoch: 0.8661268269132685\n",
            "Epoch 20......Step: 2800/3000....... Average Loss for Epoch: 0.846381642307554\n",
            "Epoch 20......Step: 2900/3000....... Average Loss for Epoch: 0.8281291895796513\n",
            "Epoch 20......Step: 3000/3000....... Average Loss for Epoch: 0.8112201942503452\n",
            "Epoch 20/20 Done, Total Loss: 0.8112201942503452\n",
            "Total Training Time: 414.7318334579468 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8INN6be1H2J"
      },
      "source": [
        "train_accuracy, train_outputs, train_results = mlp_evaluate(mlp_model, train_loader)\n",
        "test_accuracy, train_outputs, train_results = mlp_evaluate(mlp_model, test_loader)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuYd3JS-1d5N",
        "outputId": "f349100a-e4f9-4cae-dd91-97e43e6248b6"
      },
      "source": [
        "print(train_accuracy)\n",
        "print(test_accuracy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5030)\ntensor(0.4997)\n"
          ]
        }
      ]
    }
  ]
}