{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd0a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65",
      "display_name": "Python 3.8.2 64-bit ('HW3': venv)"
    },
    "metadata": {
      "interpreter": {
        "hash": "a94883f8812057f41bf7c0f7649a41127778eaf25b6b91e0966940eb42cb8f65"
      }
    },
    "colab": {
      "name": "Copy of problem_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb3v-7vPca6E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import Module, GRU, Embedding, Linear, Sigmoid, CrossEntropyLoss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2vJl0i5ca6V"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHjwy_3zcpui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000bd502-69fa-4ec4-f6c8-d43b4b00adc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrqY84fca6V"
      },
      "source": [
        "\"data/sentiment_analysis/train_pos_merged.txt\"\n",
        "# function clearing HTML tags from text\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "# preprocessing\n",
        "def clean_text(path):\n",
        "    reviews = []\n",
        "    all_words = []\n",
        "    with open(path) as pos:\n",
        "        lines = pos.readlines()\n",
        "        for line in lines:\n",
        "            #clear html tags\n",
        "            line = cleanhtml(line)\n",
        "            # lower case and punctuation\n",
        "            line = re.sub(r'[^a-zA-Z]', ' ', line.lower())\n",
        "            # split to list of words\n",
        "            words = line.split()\n",
        "            # add list to reviews\n",
        "            reviews.append(words)\n",
        "            # extend words with new review\n",
        "            all_words.extend(words)\n",
        "\n",
        "    return reviews, all_words\n",
        "\n",
        "def create_vocab(words):\n",
        "    # create vocabulary with indexes\n",
        "    vocab = {}\n",
        "    id = 1\n",
        "    for word in words:\n",
        "        if word not in vocab.keys():\n",
        "            vocab[word] = id\n",
        "            id += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def vectorize_data(reviews, y, vocab, LENGTH=400):\n",
        "    y = np.array([y for _ in range(len(reviews))])\n",
        "    indexed_reviews = np.zeros((len(reviews), LENGTH), dtype = np.int64)\n",
        "    for i, review in enumerate(reviews):\n",
        "        indexed_review = []\n",
        "        for word in review:\n",
        "            indexed_review.append(vocab[word])\n",
        "        indexed_reviews[i, max(LENGTH-len(review),0):] = indexed_review[:400]\n",
        "    return indexed_reviews, y\n",
        "\n",
        "def preprocessing(path1, path2, y1, y2, vocab, LENGTH=400):\n",
        "    reviews1, words1 = clean_text(path1)\n",
        "    reviews2, words2 = clean_text(path2)\n",
        "    # words1.extend(words2)\n",
        "    # print(words1)\n",
        "\n",
        "    del words1, words2\n",
        "\n",
        "    # vocab = create_vocab(words1)\n",
        "\n",
        "    x1, y1 = vectorize_data(reviews1, y1, vocab, LENGTH)\n",
        "    x2, y2 = vectorize_data(reviews2, y2, vocab, LENGTH)\n",
        "\n",
        "    x = np.concatenate((x1, x2))\n",
        "    y = np.concatenate((y1, y2))\n",
        "\n",
        "    return x, y #, vocab\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "lRDL7nvqca6W"
      },
      "source": [
        "# !ls /content/drive\n",
        "path = \"/content/drive/MyDrive/Deep_Learning/sentiment_analysis/\"\n",
        "reviews, words = clean_text(path + \"all_merged.txt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66iWArgxet3P"
      },
      "source": [
        "vocab = create_vocab(words)\n",
        "# vocab"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neX_8Zayfkhv"
      },
      "source": [
        "train_x, train_y = preprocessing(path + \"train_pos_merged.txt\", path + \"train_neg_merged.txt\", 0, 1, vocab)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geRNeiBuo6UP"
      },
      "source": [
        "# input = torch.from_numpy(train_x[0])\n",
        "# embedding = Embedding(len(vocab), 3, padding_idx=0)\n",
        "# embedding(input)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWOwMMSXca6Y"
      },
      "source": [
        "batch_size = 100\n",
        "train_data =  TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEaZ1Ewca6Z"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x9lfzQrca6Z"
      },
      "source": [
        "class GRU_model(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, input_dim, hidden_dim, n_layers=1, LENGTH=400):\n",
        "        \n",
        "        super(GRU_model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, input_dim, padding_idx=0)\n",
        "        self.gru = GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_dim, 2)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.gru(x, h)\n",
        "        # print(f\"shape of x: {x.shape}; shape of h: {h.shape}; shape of x[:,-1]: {x[:,-1].shape}\")\n",
        "        x = self.linear(x[:,-1])\n",
        "        # print(f\"shape of x: {x.shape}\")\n",
        "        x = self.sigmoid(x)\n",
        "        return x, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZOcAlKvca6Z"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_fQL4Gca6a"
      },
      "source": [
        "def gru_train(train_loader, vocab_size, learn_rate, input_dim=10, hidden_dim=16, EPOCHS=5):\n",
        "    \n",
        "    # Setting common hyperparameters\n",
        "    # input_dim = next(iter(train_loader))[0].shape[1]\n",
        "    # print(next(iter(train_loader))[0].shape[1])\n",
        "    output_dim = 1\n",
        "    n_layers = 1\n",
        "    # Instantiating the model\n",
        "    model = GRU_model(vocab_size, input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training\")\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.time()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            h = h.data\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out, h = model(x.to(device), h)\n",
        "            # print(f\"shape of out.squeeze(): {out.squeeze().shape}; shape of label: {label.shape}\")\n",
        "            loss = criterion(out.squeeze(), label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%100 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.time()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "\n",
        "def gru_evaluate(model, test_loader): #, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    err = 0\n",
        "    for x, label in test_loader:\n",
        "        h = model.init_hidden(test_loader.batch_size).data\n",
        "        input = x.to(device)\n",
        "        output, h_out = model(input, h)\n",
        "        result = torch.argmax(output, dim=1)\n",
        "        results.append(result)\n",
        "        err += torch.abs(result.to(device) - label.to(device)).sum()\n",
        "    accuracy = 1 - err/len(test_loader)\n",
        "    return accuracy, outputs, results"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyld89ckca6b"
      },
      "source": [
        "gru_model = gru_train(\n",
        "    train_loader, \n",
        "    vocab_size = len(vocab), \n",
        "    learn_rate=0.0005, \n",
        "    hidden_dim=32, \n",
        "    EPOCHS=300\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEjXCRTp6U3W"
      },
      "source": [
        "test_x, test_y = preprocessing(path + \"test_pos_merged.txt\", path + \"test_neg_merged.txt\", 0, 1, vocab)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC0Zdgdj7Lz5"
      },
      "source": [
        "batch_size = 100\n",
        "test_data =  TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "test_loader = DataLoader(test_data) #, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIJ2sIbz7hGH"
      },
      "source": [
        "accuracy, outputs, results = gru_evaluate(gru_model, test_loader)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O08JCDG7Ct4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee858c9-c457-49f9-dff0-ec721344eccc"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6850, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRS6VyanwsKT"
      },
      "source": [
        "## **Part 3** MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQs5x3q7wq_q"
      },
      "source": [
        "class MLP_model(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, input_dim, hidden_dim, LENGTH = 400):\n",
        "        \n",
        "        super(MLP_model, self).__init__()\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, input_dim, padding_idx=0)\n",
        "        self.fc1 = Linear(input_dim*LENGTH, hidden_dim)\n",
        "        self.fc2 = Linear(hidden_dim, 2)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVLN3-9xo5w"
      },
      "source": [
        "def mlp_train(train_loader, vocab_size, learn_rate, input_dim=10, hidden_dim=16, EPOCHS=5):\n",
        "    \n",
        "    # Instantiating the model\n",
        "    model = MLP_model(vocab_size, input_dim, hidden_dim)\n",
        "    # print(model)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training\")\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.time()\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out = model(x.to(device))\n",
        "            # print(out.shape)\n",
        "            # print(f\"shape of out: {out.shape}; shape of label: {label.shape}\")\n",
        "            # return 0\n",
        "            loss = criterion(out, label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%100 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.time()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        # print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "\n",
        "def mlp_evaluate(model, test_loader): \n",
        "    outputs = []\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    err = 0\n",
        "    for x, label in test_loader:\n",
        "        input = x.to(device)\n",
        "        output = model(input)\n",
        "        result = torch.argmax(output, dim=1)\n",
        "        results.append(result)\n",
        "        outputs.append(output)\n",
        "        err += torch.abs(result.to(device) - label.to(device)).sum()\n",
        "    accuracy = 1 - err/len(test_loader)\n",
        "    return accuracy, outputs, results"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg0KFulFyPiQ",
        "outputId": "844c7c20-4e91-47b0-e4f5-a345a2ea7f1e"
      },
      "source": [
        "mlp_model = mlp_train(train_loader, len(vocab), 0.01, hidden_dim=100 ,EPOCHS=20)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training\n",
            "Epoch 1/20 Done, Total Loss: 0.6985131442546845\n",
            "Epoch 2/20 Done, Total Loss: 0.5119846334060033\n",
            "Epoch 3/20 Done, Total Loss: 0.39750262002150216\n",
            "Epoch 4/20 Done, Total Loss: 0.36384477814038596\n",
            "Epoch 5/20 Done, Total Loss: 0.34803938269615176\n",
            "Epoch 6/20 Done, Total Loss: 0.34036123057206474\n",
            "Epoch 7/20 Done, Total Loss: 0.3342536409695943\n",
            "Epoch 8/20 Done, Total Loss: 0.33214935262997947\n",
            "Epoch 9/20 Done, Total Loss: 0.3303663025299708\n",
            "Epoch 10/20 Done, Total Loss: 0.329814816514651\n",
            "Epoch 11/20 Done, Total Loss: 0.32886905074119566\n",
            "Epoch 12/20 Done, Total Loss: 0.32853820323944094\n",
            "Epoch 13/20 Done, Total Loss: 0.3276657889286677\n",
            "Epoch 14/20 Done, Total Loss: 0.3271322637796402\n",
            "Epoch 15/20 Done, Total Loss: 0.3267263243595759\n",
            "Epoch 16/20 Done, Total Loss: 0.32620028456052147\n",
            "Epoch 17/20 Done, Total Loss: 0.3259075492620468\n",
            "Epoch 18/20 Done, Total Loss: 0.3252989202737808\n",
            "Epoch 19/20 Done, Total Loss: 0.3249521404504776\n",
            "Epoch 20/20 Done, Total Loss: 0.3249472051858902\n",
            "Total Training Time: 1.5855982303619385 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8INN6be1H2J"
      },
      "source": [
        "accuracy, outputs, results = mlp_evaluate(mlp_model, test_loader)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuYd3JS-1d5N",
        "outputId": "f349100a-e4f9-4cae-dd91-97e43e6248b6"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5467, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}